# Mission d'accompagnement Claude - Projet Penny

## üéØ Objectif de la mission

Accompagner la cr√©ation d'un projet ETL open-source permettant d'extraire les donn√©es Pennylane, de les traiter et de les rendre disponibles pour Power BI avec des performances optimales et une fr√©quence de rafra√Æchissement √©lev√©e.

## üìã Probl√©matiques initiales

### 1. Performance Power BI
- **Constat** : Temps de rafra√Æchissement de 30-60 minutes
- **Cause** : Transformations lourdes dans Power Query sur des millions de lignes
- **Solution propos√©e** : D√©porter les transformations en Python/PostgreSQL

### 2. Fr√©quence de rafra√Æchissement
- **Constat** : API Pennylane avec cache de 2 heures
- **Besoin** : Donn√©es quasi temps-r√©el pendant les p√©riodes de r√©vision comptable intensive
- **Solution propos√©e** : Scheduler Python avec rafra√Æchissement toutes les 10 minutes

## üèóÔ∏è Architecture d√©velopp√©e

```
Pennylane API/Redshift
        ‚Üì
    Python ETL (toutes les 10 min)
        ‚Üì
    PostgreSQL (donn√©es trait√©es)
        ‚Üì
    Power BI / Jupyter
```

## üìÇ Structure du projet cr√©√©e

```
Penny/
‚îú‚îÄ‚îÄ .env                          # Credentials (non commit√©)
‚îú‚îÄ‚îÄ .env.example                  # Template credentials
‚îú‚îÄ‚îÄ docker-compose.yml            # PostgreSQL + pgAdmin
‚îú‚îÄ‚îÄ requirements.txt              # D√©pendances Python
‚îú‚îÄ‚îÄ config.json                   # Configuration endpoints
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ config_loader.py         # Chargement configuration
‚îÇ   ‚îú‚îÄ‚îÄ pennylane_api.py         # Client API avec rate limiting
‚îÇ   ‚îú‚îÄ‚îÄ database.py              # Op√©rations PostgreSQL
‚îÇ   ‚îú‚îÄ‚îÄ transformations.py       # Transformations donn√©es
‚îÇ   ‚îú‚îÄ‚îÄ main.py                  # Orchestration ETL
‚îÇ   ‚îî‚îÄ‚îÄ scheduler.py             # Automatisation 10 min
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îî‚îÄ‚îÄ import_list_table.ipynb  # Exploration Redshift
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îî‚îÄ‚îÄ test_redshift.py         # Test connexion Redshift
‚îú‚îÄ‚îÄ docs/                        # Documentation MkDocs
‚îî‚îÄ‚îÄ .github/workflows/           # CI/CD GitHub Actions
```

## üöÄ √âtapes de r√©alisation

### Phase 1 : Recherche et architecture (Messages 1-15)
1. ‚úÖ Recherche documentation API Pennylane
2. ‚úÖ Analyse probl√®mes performance Power BI
3. ‚úÖ Proposition architecture ETL compl√®te
4. ‚úÖ Validation des besoins utilisateur (volume, fr√©quence, transformations)
5. ‚úÖ Identification contraintes techniques (rate limiting API)

### Phase 2 : D√©veloppement core ETL (Messages 16-40)
1. ‚úÖ Configuration s√©curis√©e avec `.env` et `.env.example`
2. ‚úÖ Setup Docker PostgreSQL + pgAdmin
3. ‚úÖ Client API avec rate limiting (4.5 req/sec)
4. ‚úÖ Gestion pagination API
5. ‚úÖ Module base de donn√©es avec batch loading
6. ‚úÖ Orchestration ETL (mode full/incremental)
7. ‚úÖ Scheduler automatique 10 minutes
8. ‚úÖ Documentation utilisateur compl√®te

### Phase 3 : GitHub et d√©ploiement (Messages 41-55)
1. ‚úÖ Cr√©ation repository GitHub "Penny"
2. ‚úÖ Documentation MkDocs avec 8 pages
3. ‚úÖ GitHub Actions CI/CD
4. ‚úÖ D√©ploiement GitHub Pages
5. ‚úÖ Badges professionnels README
6. ‚úÖ Guide utilisateur √©volutif

### Phase 4 : Setup PostgreSQL (Messages 56-65)
1. ‚úÖ Configuration `.env` avec credentials Pennylane
2. ‚úÖ R√©solution conflit port (5432 ‚Üí 5433)
3. ‚úÖ Lancement Docker Compose
4. ‚úÖ V√©rification cr√©ation base et sch√©mas
5. ‚úÖ Validation PostgreSQL op√©rationnel

### Phase 5 : Clarification acc√®s Pennylane (Messages 66-75)
1. ‚úÖ Identification 2 m√©thodes d'acc√®s distinctes :
   - **Data Sharing** : Acc√®s SQL direct au Data Warehouse Redshift
   - **API REST** : API avec scopes et tokens d√©veloppeurs
2. ‚úÖ S√©paration tokens dans `.env` :
   - `PENNYLANE_DATA_SHARING_KEY` pour Redshift
   - `PENNYLANE_API_TOKEN` pour REST API
3. ‚úÖ Test connexion Redshift r√©ussi
4. ‚úÖ Validation acc√®s Data Warehouse fonctionnel

### Phase 6 : Jupyter Notebooks (Messages 76-83)
1. ‚úÖ Cr√©ation `import_list_table.ipynb`
2. ‚úÖ **Apprentissage structure notebook** : alternance markdown/code/display
3. ‚úÖ Code exploration tables Redshift
4. ‚úÖ Configuration environnement Python 3.12.10
5. ‚úÖ Installation pyarrow
6. ‚úÖ Correction requ√™te SQL (`information_schema.tables`)

## üìö Apprentissages cl√©s

### 1. Double acc√®s Pennylane
**D√©couverte importante** : Pennylane propose 2 m√©thodes d'acc√®s diff√©rentes :
- **Redshift Data Sharing** : Acc√®s direct SQL au Data Warehouse (port 5439)
- **API REST** : API HTTP avec rate limiting et pagination

**Impact** : N√©cessit√© de s√©parer les deux tokens et de clarifier l'usage selon le besoin.

### 2. Structure Jupyter Notebooks
**Feedback utilisateur critique** :
> "l'int√©r√™t de jupyter notebook c'est que je puisse visualiser. et donc en fonction retraiter. il faut donc qu'il y ait plusieurs print en sortie afin de visualiser chaque √©tape."

**Apprentissage** :
- ‚ùå **Erreur initiale** : Regrouper tout le code ensemble puis tous les affichages
- ‚úÖ **Bonne pratique** : Alterner Markdown ‚Üí Code ‚Üí Display ‚Üí Markdown ‚Üí Code ‚Üí Display
- **Raison** : Permet une visualisation et un ajustement progressif √† chaque √©tape

### 3. Permissions Redshift
**Probl√®me rencontr√©** : `permission denied for relation pg_tables`

**Apprentissage** :
- Les vues syst√®me PostgreSQL (`pg_tables`, `pg_views`) ne sont pas toujours accessibles sur Redshift
- **Solution** : Utiliser `information_schema.tables` qui est accessible par d√©faut

### 4. Mots r√©serv√©s SQL
**Erreur** : `syntax error at or near "table"`

**Apprentissage** :
- `table` est un mot r√©serv√© en SQL
- **Solutions** :
  1. Renommer la colonne (`table` ‚Üí `table_name`)
  2. Entourer de guillemets doubles (`"table"`)

### 5. Gestion ports Docker sur Windows
**Probl√®me** : Port 5432 d√©j√† utilis√© par un autre conteneur PostgreSQL

**Apprentissage** :
- V√©rifier les ports utilis√©s avec `docker ps`
- Configurer port alternatif via variables d'environnement
- Utiliser `.env` pour rendre la configuration flexible

### 6. Encodage Windows et emojis
**Probl√®me** : `UnicodeEncodeError: 'charmap' codec can't encode character`

**Apprentissage** :
- Console Windows ne supporte pas tous les emojis
- **Solution** : Remplacer par indicateurs texte (`[OK]`, `[ERREUR]`)

### 7. Installation packages Jupyter
**Avertissement** : `Defaulting to user installation because normal site-packages is not writeable`

**Apprentissage** :
- Installation au niveau utilisateur est normale sur Windows
- N√©cessite red√©marrage du kernel Jupyter pour prise en compte
- Utiliser `Note: you may need to restart the kernel` comme signal

## üîÑ Workflow de travail √©tabli

### D√©veloppement ETL automatis√© (original)
```
1. Extraction API/Redshift
2. Transformations Python
3. Chargement PostgreSQL
4. Connexion Power BI
```

### Workflow utilisateur final (d√©couvert)
```
1. Extraction manuelle via Jupyter Notebooks
2. Visualisation progressive des donn√©es
3. Retraitement it√©ratif selon r√©sultats
4. Transformations adapt√©es aux besoins sp√©cifiques
```

**Impact** : L'utilisateur pr√©f√®re contr√¥ler manuellement l'extraction via notebooks plut√¥t qu'utiliser l'ETL automatis√©. Le projet devient une **bo√Æte √† outils** plut√¥t qu'un pipeline automatis√©.

## üõ†Ô∏è Technologies utilis√©es

- **Python 3.12** : Langage principal
- **PostgreSQL 15** : Base de donn√©es (via Docker)
- **Redshift** : Data Warehouse Pennylane
- **Jupyter Notebooks** : Exploration et transformation interactive
- **Docker Compose** : Orchestration conteneurs
- **python-dotenv** : Gestion credentials
- **psycopg2** : Connexion PostgreSQL/Redshift
- **pandas** : Manipulation donn√©es
- **pyarrow** : Performance pandas
- **MkDocs Material** : Documentation
- **GitHub Actions** : CI/CD

## üìä D√©cisions techniques cl√©s

| D√©cision | Raison |
|----------|--------|
| PostgreSQL 15 au lieu de 16 | Image d√©j√† en cache localement |
| Port 5433 au lieu de 5432 | √âviter conflit avec PostgreSQL existant |
| Rate limiting √† 4.5 req/sec | Marge de s√©curit√© sous limite API (5 req/sec) |
| Batch size 1000 lignes | Compromis performance/m√©moire |
| 2 tokens s√©par√©s | Clarifier usage Redshift vs API REST |
| `information_schema.tables` | Permissions Redshift limit√©es |
| Python 3.12.10 Global Env | Coh√©rence avec installation syst√®me |

## üéì M√©thodologie d'accompagnement

### Principes suivis
1. **Dialogue avant action** : Clarifier besoins avant d√©velopper
2. **It√©ratif** : Ajuster selon retours utilisateur
3. **Documentation compl√®te** : Projet partageable et r√©utilisable
4. **S√©curit√©** : `.env` non commit√©, `.env.example` fourni
5. **Standards professionnels** : CI/CD, tests, documentation

### Adaptation au feedback
- **Modification structure notebooks** suite retour utilisateur
- **S√©paration tokens** apr√®s clarification usage
- **Changement port PostgreSQL** selon infrastructure existante
- **Passage d'ETL automatis√© √† bo√Æte √† outils** selon workflow pr√©f√©r√©

## üîÆ Perspectives futures

### Court terme
- [ ] Finaliser exploration tables Redshift dans notebook
- [ ] Cr√©er notebooks suppl√©mentaires pour autres endpoints
- [ ] Documenter requ√™tes SQL r√©utilisables

### Moyen terme
- [ ] Partage public du repository GitHub
- [ ] Contributions communautaires possibles
- [ ] Enrichissement documentation avec cas d'usage

### Long terme
- [ ] √âventuellement r√©activer ETL automatis√© si besoin
- [ ] Int√©gration d'autres sources comptables
- [ ] Dashboards Power BI pr√©construits

## üìù Notes importantes

### Variables d'environnement sensibles
```env
PENNYLANE_DATA_SHARING_KEY=***  # Redshift access
PENNYLANE_API_TOKEN=***         # REST API access
POSTGRES_PASSWORD=***           # PostgreSQL password
```

**‚ö†Ô∏è Ne jamais commiter `.env` sur GitHub**

### Commandes essentielles

```bash
# D√©marrer PostgreSQL
docker-compose up -d

# Installer d√©pendances Python
pip install -r requirements.txt

# Tester connexion Redshift
python tests/test_redshift.py

# Lancer Jupyter
jupyter notebook
```

## ü§ù Collaboration

Le projet est con√ßu pour √™tre **open-source et partageable**. Toute contribution future devra :
- Respecter la structure √©tablie
- Documenter les changements
- Tester avant commit
- Suivre les standards du projet

---

**Date de cr√©ation** : 2025-10-07
**Derni√®re mise √† jour** : 2025-10-07
**Statut** : En d√©veloppement actif
